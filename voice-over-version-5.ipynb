{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11525193,"sourceType":"datasetVersion","datasetId":7228093},{"sourceId":11720374,"sourceType":"datasetVersion","datasetId":7357459}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install dependencies","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y tortoise-tts tortoise tokenizers transformers \n\n!pip install tokenizers==0.13.3\n!pip install transformers==4.30.2\n!pip install torchaudio\n\n!pip install unidecode phonemizer inflect gradio tqdm matplotlib librosa\n\nprint(\"Installing simpler voice synthesis components...\")\n\n!pip install fairseq\n!pip install pydub\n!pip install datasets soundfile\n\nprint(\"Dependencies installed successfully!\")\nprint(\"\\nProceeding with simplified voice cloning implementation...\")\n\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"Warning: No GPU detected. This may significantly slow down the process.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:08:19.3003Z","iopub.execute_input":"2025-05-07T18:08:19.300887Z","iopub.status.idle":"2025-05-07T18:10:38.51593Z","shell.execute_reply.started":"2025-05-07T18:08:19.300864Z","shell.execute_reply":"2025-05-07T18:10:38.51511Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import Modules","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport numpy as np\nimport librosa\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport random\nimport json\nimport re\nimport matplotlib.pyplot as plt\nimport warnings\nfrom zipfile import ZipFile\nimport subprocess\nimport json\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:13:53.772944Z","iopub.execute_input":"2025-05-07T18:13:53.773732Z","iopub.status.idle":"2025-05-07T18:13:54.660024Z","shell.execute_reply.started":"2025-05-07T18:13:53.773708Z","shell.execute_reply":"2025-05-07T18:13:54.659206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install Dependecies x2","metadata":{}},{"cell_type":"code","source":"\n# Install necessary libraries\ndef install_dependencies():\n    print(\"Installing dependencies...\")\n    packages = [\n        \"transformers\",\n        \"datasets\",\n        \"soundfile\",\n        \"fairseq\",\n        \"gradio\",\n        \"pydub\"\n    ]\n    \n    for package in packages:\n        subprocess.run(f\"pip install {package}\", shell=True)\n    \n    # Install fairseq for TTS capabilities\n    subprocess.run(\"pip install git+https://github.com/facebookresearch/fairseq.git\", shell=True)\n    \n    print(\"Dependencies installed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:13:58.358059Z","iopub.execute_input":"2025-05-07T18:13:58.35834Z","iopub.status.idle":"2025-05-07T18:13:58.363128Z","shell.execute_reply.started":"2025-05-07T18:13:58.358319Z","shell.execute_reply":"2025-05-07T18:13:58.362348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"install_dependencies()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:14:01.692518Z","iopub.execute_input":"2025-05-07T18:14:01.692851Z","iopub.status.idle":"2025-05-07T18:17:04.652885Z","shell.execute_reply.started":"2025-05-07T18:14:01.692781Z","shell.execute_reply":"2025-05-07T18:17:04.652121Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create lyrics.json File","metadata":{}},{"cell_type":"code","source":"def create_lyrics_json(txt_folder, wav_folder, output_file=\"lyrics.json\"):\n    \n    song_names = {os.path.splitext(f)[0] for f in os.listdir(wav_folder) if f.endswith(\".wav\")}\n\n    lyrics_dict = {}\n\n    for f in os.listdir(txt_folder):\n        if f.endswith(\".txt\"):\n            base_name = os.path.splitext(f)[0]\n            if base_name in song_names:\n                with open(os.path.join(txt_folder, f), 'r', encoding='utf-8') as file:\n                    lyrics = file.read().strip()\n                    lyrics_dict[base_name] = lyrics\n\n    with open(output_file, 'w', encoding='utf-8') as json_file:\n        json.dump(lyrics_dict, json_file, indent=4, ensure_ascii=False)\n\n    print(f\"Lyrics JSON saved to {output_file}\")\n\n\ncreate_lyrics_json(txt_folder=\"/kaggle/input/lyrics\", wav_folder=\"/kaggle/input/audio-dataset\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:19:02.570766Z","iopub.execute_input":"2025-05-07T18:19:02.571315Z","iopub.status.idle":"2025-05-07T18:19:02.614665Z","shell.execute_reply.started":"2025-05-07T18:19:02.571291Z","shell.execute_reply":"2025-05-07T18:19:02.613996Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preparation functions","metadata":{}},{"cell_type":"code","source":"def prepare_data(wav_dir, lyrics_file, output_dir=\"dataset\"):\n    \"\"\"\n    Prepare dataset for training by organizing audio files and corresponding lyrics\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Read lyrics file\n    with open(lyrics_file, 'r', encoding='utf-8') as f:\n        lyrics_data = json.load(f)\n    \n    # Create dataset structure\n    dataset = []\n    wav_files = [f for f in os.listdir(wav_dir) if f.endswith('.wav')]\n    \n    for wav_file in tqdm(wav_files, desc=\"Processing audio files\"):\n        song_id = os.path.splitext(wav_file)[0]\n        \n        if song_id in lyrics_data:\n            # Process audio\n            audio_path = os.path.join(wav_dir, wav_file)\n            \n            # Extract audio features\n            y, sr = librosa.load(audio_path, sr=16000)\n            \n            # Save processed audio\n            processed_audio_path = os.path.join(output_dir, f\"{song_id}.wav\")\n            torchaudio.save(processed_audio_path, torch.tensor(y).unsqueeze(0), sr)\n            \n            # Save lyrics\n            lyrics = lyrics_data[song_id]\n            \n            # Add to dataset\n            dataset.append({\n                \"id\": song_id,\n                \"audio_path\": processed_audio_path,\n                \"lyrics\": lyrics\n            })\n    \n    # Save dataset info\n    with open(os.path.join(output_dir, \"dataset_info.json\"), 'w', encoding='utf-8') as f:\n        json.dump(dataset, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Dataset prepared with {len(dataset)} songs\")\n    return dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:19:14.385301Z","iopub.execute_input":"2025-05-07T18:19:14.385593Z","iopub.status.idle":"2025-05-07T18:19:14.392483Z","shell.execute_reply.started":"2025-05-07T18:19:14.385561Z","shell.execute_reply":"2025-05-07T18:19:14.391851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # 2. Set up paths\nwav_dir = \"/kaggle/input/audio-dataset\"\nlyrics_file = \"/kaggle/working/lyrics.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:19:18.30604Z","iopub.execute_input":"2025-05-07T18:19:18.306539Z","iopub.status.idle":"2025-05-07T18:19:18.309789Z","shell.execute_reply.started":"2025-05-07T18:19:18.306519Z","shell.execute_reply":"2025-05-07T18:19:18.309219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Prepare dataset\ndataset = prepare_data(wav_dir, lyrics_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:19:19.722099Z","iopub.execute_input":"2025-05-07T18:19:19.722586Z","iopub.status.idle":"2025-05-07T18:19:54.286166Z","shell.execute_reply.started":"2025-05-07T18:19:19.722558Z","shell.execute_reply":"2025-05-07T18:19:54.2855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Voice Cloner Class\n","metadata":{}},{"cell_type":"code","source":"class SimpleUrduVoiceCloner:\n    def __init__(self, dataset_dir, voice_samples_dir=\"voice_samples\"):\n        \"\"\"\n        Initialize the voice cloner model with simpler components\n        \"\"\"\n        self.dataset_dir = dataset_dir\n        self.voice_samples_dir = voice_samples_dir\n        os.makedirs(self.voice_samples_dir, exist_ok=True)\n        \n        try:\n            with open(os.path.join(dataset_dir, \"dataset_info.json\"), 'r', encoding='utf-8') as f:\n                self.dataset = json.load(f)\n        except FileNotFoundError:\n            print(f\"Warning: Could not find dataset info at {dataset_dir}/dataset_info.json\")\n            self.dataset = []\n        \n        self._prepare_voice_samples()\n        \n        self._load_model()\n        \n        print(\"Voice cloner initialized successfully!\")\n    \n    def _prepare_voice_samples(self):\n        \"\"\"\n        Prepare voice samples for cloning\n        \"\"\"\n        sample_count = min(5, len(self.dataset))\n        if sample_count == 0:\n            print(\"Warning: No samples found in dataset\")\n            return\n            \n        samples = random.sample(self.dataset, sample_count)\n        \n        for idx, sample in enumerate(samples):\n            src_path = sample[\"audio_path\"]\n            dst_path = os.path.join(self.voice_samples_dir, f\"sample_{idx}.wav\")\n            \n            # Copy the file\n            if os.path.exists(src_path):\n                y, sr = librosa.load(src_path, sr=16000)\n                if len(y) > sr * 10:\n                    y = y[:sr * 10]\n                \n                torchaudio.save(dst_path, torch.tensor(y).unsqueeze(0), sr)\n        \n        print(f\"Prepared {sample_count} voice samples for cloning\")\n    \n    def _load_model(self):\n        \"\"\"\n        Load the pre-trained model\n        \"\"\"\n        try:\n            from fairseq.checkpoint_utils import load_model_ensemble_and_task\n            from fairseq.models.text_to_speech.hub_interface import TTSHubInterface\n            \n            print(\"Loading pre-trained TTS model...\")\n            self.use_fairseq = True\n            \n            model_path = \"tts_model\"\n            if not os.path.exists(model_path):\n                os.makedirs(model_path, exist_ok=True)\n                print(\"Downloading TTS model... (this may take a while)\")\n                subprocess.run(\"wget -q https://dl.fbaipublicfiles.com/fairseq/s2/mustc_en_de_transformer_plus_hubert_large_ll60k.pt -O tts_model/model.pt\", shell=True)\n            \n            try:\n                models, cfg, task = load_model_ensemble_and_task([\"tts_model/model.pt\"])\n                self.model = TTSHubInterface(cfg, task, models)\n                print(\"Model loaded successfully!\")\n            except Exception as e:\n                print(f\"Error loading fairseq model: {e}\")\n                self.use_fairseq = False\n                self._load_backup_model()\n        except ImportError:\n            print(\"fairseq not available, falling back to simpler model\")\n            self.use_fairseq = False\n            self._load_backup_model()\n    \n    def _load_backup_model(self):\n        \"\"\"\n        Load a simpler backup model using transformers library\n        \"\"\"\n        try:\n            from transformers import AutoProcessor, AutoModel\n            \n            # Using a lightweight pretrained model\n            print(\"Loading backup TTS model...\")\n            self.processor = AutoProcessor.from_pretrained(\"facebook/mms-tts-hin\")\n            self.model = AutoModel.from_pretrained(\"facebook/mms-tts-hin\")\n            print(\"Backup model loaded successfully!\")\n        except Exception as e:\n            print(f\"Error loading backup model: {e}\")\n            print(\"Will use audio concatenation method instead\")\n            self.model = None\n    \n    def generate_song(self, lyrics, output_path=\"generated_song.wav\", voice_name=\"atif_aslam\"):\n        \"\"\"\n        Generate a song with the given lyrics in Atif Aslam's voice\n        \"\"\"\n        cleaned_lyrics = self._preprocess_lyrics(lyrics)\n        print(f\"Generating song with lyrics: {cleaned_lyrics[:50]}...\")\n        \n        if hasattr(self, 'use_fairseq') and self.use_fairseq and self.model is not None:\n            try:\n                audio = self.model.synthesize_speech(cleaned_lyrics)\n                torchaudio.save(output_path, audio.unsqueeze(0), 16000)\n                return output_path\n            except Exception as e:\n                print(f\"Error with fairseq TTS: {e}\")\n                print(\"Falling back to backup method\")\n        \n        if hasattr(self, 'processor') and self.model is not None:\n            try:\n                inputs = self.processor(text=cleaned_lyrics, return_tensors=\"pt\")\n                with torch.no_grad():\n                    output = self.model(**inputs).waveform\n                torchaudio.save(output_path, output, 16000)\n                return output_path\n            except Exception as e:\n                print(f\"Error with transformer TTS: {e}\")\n                print(\"Falling back to concatenative synthesis\")\n        \n        print(\"Using concatenative synthesis method\")\n        self._generate_with_concatenation(cleaned_lyrics, output_path)\n        \n        print(f\"Song generated and saved to {output_path}\")\n        return output_path\n    \n    def _generate_with_concatenation(self, lyrics, output_path):\n        \"\"\"\n        Generate audio using simple concatenation from samples\n        \"\"\"\n        from pydub import AudioSegment\n        \n        samples = []\n        for i in range(5):\n            sample_path = os.path.join(self.voice_samples_dir, f\"sample_{i}.wav\")\n            if os.path.exists(sample_path):\n                audio = AudioSegment.from_wav(sample_path)\n                chunk_length = 200  \n                for i in range(0, len(audio), chunk_length):\n                    chunk = audio[i:i+chunk_length]\n                    if len(chunk) >= 100:\n                        samples.append(chunk)\n        \n        if not samples:\n            print(\"No samples available for concatenation\")\n            return\n            \n        words = re.split(r'\\s+', lyrics)\n        output = AudioSegment.empty()\n        \n        for word in words:\n            num_chunks = max(1, len(word) // 2)\n            for _ in range(num_chunks):\n                chunk = random.choice(samples)\n                output += chunk\n            \n            output += AudioSegment.silent(duration=50)\n        \n        output.export(output_path, format=\"wav\")\n    \n    def _preprocess_lyrics(self, lyrics):\n        \"\"\"\n        Preprocess Urdu Roman lyrics for TTS\n        \"\"\"\n        cleaned = re.sub(r'[^\\w\\s]', ' ', lyrics)\n        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n        \n        return cleaned\n    \n    def fine_tune(self, epochs=2):\n        \"\"\"\n        Simplified fine-tuning approach\n        \"\"\"\n        print(\"Starting simplified adaptation process...\")\n        if len(self.dataset) == 0:\n            print(\"No data available for fine-tuning\")\n            return\n            \n        print(\"Analyzing voice characteristics...\")\n        \n        all_features = []\n        for sample in tqdm(self.dataset[:5], desc=\"Analyzing voice samples\"):\n            try:\n                audio_path = sample[\"audio_path\"]\n                y, sr = librosa.load(audio_path, sr=16000)\n                \n                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n                spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n                \n                all_features.append({\n                    \"mfccs\": np.mean(mfccs, axis=1),\n                    \"centroid\": np.mean(spectral_centroid)\n                })\n            except Exception as e:\n                print(f\"Error processing sample {sample['id']}: {e}\")\n        \n        if all_features:\n            print(\"Voice characteristics analyzed\")\n            \n            plt.figure(figsize=(10, 4))\n            for i, features in enumerate(all_features):\n                plt.plot(features[\"mfccs\"], label=f\"Sample {i+1}\")\n            plt.title(\"Voice MFCC Profiles\")\n            plt.legend()\n            plt.savefig(\"voice_profile.png\")\n            print(\"Voice profile visualization saved to voice_profile.png\")\n        \n        print(\"Adaptation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:20:14.06639Z","iopub.execute_input":"2025-05-07T18:20:14.067125Z","iopub.status.idle":"2025-05-07T18:20:14.089305Z","shell.execute_reply.started":"2025-05-07T18:20:14.067097Z","shell.execute_reply":"2025-05-07T18:20:14.088515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"voice_cloner = SimpleUrduVoiceCloner(\"dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:20:19.8436Z","iopub.execute_input":"2025-05-07T18:20:19.843988Z","iopub.status.idle":"2025-05-07T18:20:24.087484Z","shell.execute_reply.started":"2025-05-07T18:20:19.843964Z","shell.execute_reply":"2025-05-07T18:20:24.086692Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"voice_cloner.fine_tune(epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:20:31.3393Z","iopub.execute_input":"2025-05-07T18:20:31.340141Z","iopub.status.idle":"2025-05-07T18:20:38.998118Z","shell.execute_reply.started":"2025-05-07T18:20:31.340118Z","shell.execute_reply":"2025-05-07T18:20:38.997203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Song Generation","metadata":{}},{"cell_type":"code","source":"import IPython.display as ipd\nimport torchaudio\nimport matplotlib.pyplot as plt\n\nsufi_lyrics_list = [\n    '''Maula mere maula, dil mein samaya\nTeri yaadon ka deepak jalaaya\nHar dam tu hi tu hai, rooh mein basa hai\nIshq tera saaya, har waqt saath hai''',\n    '''Manzil tu, rehnuma tu, saaya tera har simt hai\nTere bina kuch bhi nahi, tu hi to har ek rasm hai\nIshq tera junoon sa hai, jise har dil chahta hai\nFanaa ho jaayein us mein jo tujhe paa jaata hai\n\n''',\n    '''Main hoon faqeer tera, tu de de dard mujhe\nTere dard mein bhi sukoon hai, tu bas nazar mein rahe\nNa chahiye jannat, na chahiye maal-o-daulat\nBas tu mil jaaye, yahi hai asli ne'mat\n\n''',\n    '''Ishq Allah ka bandagi hai\nJo is mein dooba, wahi faqi hai\nNa maal chahiye, na daulat ki chaaht\nBas uski yaadon ki hai chahat\n\n''',\n    '''Sajda kiya maine tere ishq mein'''\n]\n\ndef display_waveform_and_spectrogram(wav_path):\n    waveform, sr = torchaudio.load(wav_path)\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(waveform.t().numpy())\n    plt.title(\"Waveform\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Amplitude\")\n\n    mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=sr)(waveform)\n    plt.subplot(1, 2, 2)\n    plt.imshow(mel_spec.log2()[0,:,:].detach().numpy(), aspect='auto', origin='lower')\n    plt.title(\"Mel Spectrogram\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Frequency\")\n\n    plt.tight_layout()\n    plt.show()\n    \n    return ipd.Audio(wav_path)\n\nfor i, lyrics in enumerate(sufi_lyrics_list, start=1):\n    filename = f\"generated_song_{i}.wav\"\n    voice_cloner.generate_song(lyrics, filename)\n    print(f\"\\n🎵 Playing song {i}: {lyrics}\")\n    audio = display_waveform_and_spectrogram(filename)\n    display(audio)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:20:49.382751Z","iopub.execute_input":"2025-05-07T18:20:49.383263Z","iopub.status.idle":"2025-05-07T18:20:51.972404Z","shell.execute_reply.started":"2025-05-07T18:20:49.383242Z","shell.execute_reply":"2025-05-07T18:20:51.97168Z"}},"outputs":[],"execution_count":null}]}