{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11525193,"sourceType":"datasetVersion","datasetId":7228093},{"sourceId":11720374,"sourceType":"datasetVersion","datasetId":7357459}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\nimport librosa\nimport nltk\nimport soundfile as sf\nfrom sklearn.model_selection import train_test_split\nimport re\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom functools import lru_cache\nimport gc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:52:35.519823Z","iopub.execute_input":"2025-05-07T20:52:35.520476Z","iopub.status.idle":"2025-05-07T20:52:35.525503Z","shell.execute_reply.started":"2025-05-07T20:52:35.520450Z","shell.execute_reply":"2025-05-07T20:52:35.524585Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"nltk.download('averaged_perceptron_tagger_eng')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:52:35.534923Z","iopub.execute_input":"2025-05-07T20:52:35.535216Z","iopub.status.idle":"2025-05-07T20:52:35.775586Z","shell.execute_reply.started":"2025-05-07T20:52:35.535192Z","shell.execute_reply":"2025-05-07T20:52:35.774778Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Set the random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:52:35.777048Z","iopub.execute_input":"2025-05-07T20:52:35.777350Z","iopub.status.idle":"2025-05-07T20:52:35.850067Z","shell.execute_reply.started":"2025-05-07T20:52:35.777332Z","shell.execute_reply":"2025-05-07T20:52:35.849322Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Install required packages\n!pip install transformers\n!pip install pydub\n!pip install phonemizer\n!pip install g2p_en\n!pip install praatio\n!pip install pyworld\n!pip install torchtext\n!pip install unidecode\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:52:35.850776Z","iopub.execute_input":"2025-05-07T20:52:35.850978Z","iopub.status.idle":"2025-05-07T20:53:00.443678Z","shell.execute_reply.started":"2025-05-07T20:52:35.850962Z","shell.execute_reply":"2025-05-07T20:53:00.442619Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\nRequirement already satisfied: phonemizer in /usr/local/lib/python3.11/dist-packages (3.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from phonemizer) (1.4.2)\nRequirement already satisfied: segments in /usr/local/lib/python3.11/dist-packages (from phonemizer) (2.3.0)\nRequirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.11/dist-packages (from phonemizer) (25.3.0)\nRequirement already satisfied: dlinfo in /usr/local/lib/python3.11/dist-packages (from phonemizer) (2.0.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from phonemizer) (4.13.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from segments->phonemizer) (2024.11.6)\nRequirement already satisfied: csvw>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from segments->phonemizer) (3.5.1)\nRequirement already satisfied: isodate in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.7.2)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.9.0.post0)\nRequirement already satisfied: rfc3986<2 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.5.0)\nRequirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.1.1)\nRequirement already satisfied: babel in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.17.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.32.3)\nRequirement already satisfied: language-tags in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.2.0)\nRequirement already satisfied: rdflib in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (7.1.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.4.6)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.23.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->csvw>=1.5.6->segments->phonemizer) (1.17.0)\nRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2025.1.31)\nRequirement already satisfied: g2p_en in /usr/local/lib/python3.11/dist-packages (2.1.0)\nRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from g2p_en) (1.26.4)\nRequirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.11/dist-packages (from g2p_en) (3.9.1)\nRequirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from g2p_en) (7.5.0)\nRequirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from g2p_en) (0.1.3)\nRequirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect>=0.3.1->g2p_en) (10.6.0)\nRequirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=0.3.1->g2p_en) (4.4.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.4->g2p_en) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.4->g2p_en) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.4->g2p_en) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.4->g2p_en) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.1->g2p_en) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.1->g2p_en) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.1->g2p_en) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.1->g2p_en) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.1->g2p_en) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.1->g2p_en) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from typeguard>=4.0.1->inflect>=0.3.1->g2p_en) (4.13.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.13.1->g2p_en) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.13.1->g2p_en) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.13.1->g2p_en) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.13.1->g2p_en) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.13.1->g2p_en) (2024.2.0)\nRequirement already satisfied: praatio in /usr/local/lib/python3.11/dist-packages (6.2.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from praatio) (4.13.1)\nRequirement already satisfied: pyworld in /usr/local/lib/python3.11/dist-packages (0.3.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyworld) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pyworld) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyworld) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyworld) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pyworld) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pyworld) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pyworld) (2024.2.0)\nRequirement already satisfied: torchtext in /usr/local/lib/python3.11/dist-packages (0.18.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\nRequirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.5.1+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchtext) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchtext) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchtext) (2024.2.0)\nRequirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Import additional libraries after installation\nfrom transformers import AutoTokenizer, AutoModel\nfrom g2p_en import G2p\nimport pyworld as pw\nfrom pydub import AudioSegment\nimport unidecode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:00.444827Z","iopub.execute_input":"2025-05-07T20:53:00.445132Z","iopub.status.idle":"2025-05-07T20:53:04.755685Z","shell.execute_reply.started":"2025-05-07T20:53:00.445106Z","shell.execute_reply":"2025-05-07T20:53:04.755065Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AudioProcessor:\n    \"\"\"Class to process audio files and extract features\"\"\"\n    \n    def __init__(self, sr=22050, n_fft=1024, hop_length=256, n_mels=80):\n        self.sr = sr  # Sample rate\n        self.n_fft = n_fft  # FFT window size\n        self.hop_length = hop_length  # Hop length for FFT\n        self.n_mels = n_mels  # Number of mel bands\n    \n    def load_audio(self, file_path):\n        \"\"\"Load audio file and resample if necessary\"\"\"\n        audio, sr = librosa.load(file_path, sr=self.sr)\n        return audio\n    \n    def extract_mel_spectrogram(self, audio):\n        \"\"\"Extract mel spectrogram from audio\"\"\"\n        mel_spec = librosa.feature.melspectrogram(\n            y=audio,\n            sr=self.sr,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            n_mels=self.n_mels\n        )\n        # Convert to log scale (dB)\n        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        return log_mel_spec\n    \n    def extract_pitch(self, audio):\n        \"\"\"Extract pitch (f0) using PyWorld's HARVEST algorithm\"\"\"\n        # Convert audio to float64\n        audio_64 = audio.astype(np.float64)\n        \n        # Extract pitch\n        f0, t = pw.harvest(audio_64, self.sr, frame_period=self.hop_length/self.sr*1000)\n        \n        # Normalize pitch\n        f0[f0 > 0] = (f0[f0 > 0] - np.min(f0[f0 > 0])) / (np.max(f0[f0 > 0]) - np.min(f0[f0 > 0]))\n        \n        return f0\n    \n    def extract_energy(self, mel_spec):\n        \"\"\"Extract energy from mel spectrogram\"\"\"\n        energy = np.linalg.norm(mel_spec, axis=0)\n        # Normalize energy\n        energy = (energy - np.min(energy)) / (np.max(energy) - np.min(energy))\n        return energy\n    \n    def process_audio_file(self, file_path):\n        \"\"\"Process a single audio file and extract all features\"\"\"\n        audio = self.load_audio(file_path)\n        mel_spec = self.extract_mel_spectrogram(audio)\n        pitch = self.extract_pitch(audio)\n        energy = self.extract_energy(mel_spec)\n        \n        # Make sure pitch matches the number of frames in mel spectrogram\n        if len(pitch) < mel_spec.shape[1]:\n            pitch = np.pad(pitch, (0, mel_spec.shape[1] - len(pitch)))\n        elif len(pitch) > mel_spec.shape[1]:\n            pitch = pitch[:mel_spec.shape[1]]\n            \n        # Make sure energy matches the number of frames in mel spectrogram\n        if len(energy) < mel_spec.shape[1]:\n            energy = np.pad(energy, (0, mel_spec.shape[1] - len(energy)))\n        elif len(energy) > mel_spec.shape[1]:\n            energy = energy[:mel_spec.shape[1]]\n        \n        return {\n            'audio': audio,\n            'mel_spectrogram': mel_spec,\n            'pitch': pitch,\n            'energy': energy\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.757900Z","iopub.execute_input":"2025-05-07T20:53:04.758282Z","iopub.status.idle":"2025-05-07T20:53:04.767841Z","shell.execute_reply.started":"2025-05-07T20:53:04.758260Z","shell.execute_reply":"2025-05-07T20:53:04.767104Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class TextProcessor:\n    \"\"\"Class to process Roman Urdu text\"\"\"\n    \n    def __init__(self):\n        # Initialize tokenizer for Roman Urdu\n        # Since there's no specific tokenizer for Roman Urdu, \n        # we'll use a general-purpose one and fine-tune later\n        self.tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n        self.phonemizer = G2p()  # English phonemizer as a starting point\n        \n        # Map for common Roman Urdu sounds to phonemes\n        self.urdu_phoneme_map = {\n            'aa': 'ɑː',\n            'ee': 'iː',\n            'oo': 'uː',\n            'ai': 'aɪ',\n            'ae': 'eɪ',\n            'ch': 'tʃ',\n            'sh': 'ʃ',\n            'kh': 'x',\n            'gh': 'ɣ',\n            'ph': 'f',\n            'th': 'θ',\n            'dh': 'ð',\n            'ng': 'ŋ',\n            # Add more mappings as needed\n        }\n    \n    def clean_text(self, text):\n        \"\"\"Clean Roman Urdu text\"\"\"\n        # Remove special characters but keep spaces and basic punctuation\n        text = re.sub(r'[^\\w\\s\\',\\.!?]', '', text)\n        # Normalize spaces\n        text = re.sub(r'\\s+', ' ', text)\n        return text.strip()\n    \n    def text_to_sequence(self, text):\n        \"\"\"Convert text to sequence of token IDs\"\"\"\n        clean_text = self.clean_text(text)\n        tokens = self.tokenizer(clean_text, return_tensors=\"pt\")\n        return tokens['input_ids'][0]\n    \n    def get_phoneme_sequence(self, text):\n        \"\"\"Convert Roman Urdu text to approximate phoneme sequence\"\"\"\n        clean_text = self.clean_text(text)\n        \n        # Apply Roman Urdu phoneme mappings\n        for roman, phoneme in self.urdu_phoneme_map.items():\n            clean_text = clean_text.replace(roman, phoneme)\n        \n        # Use G2p for remaining text (approximation)\n        phonemes = self.phonemizer(clean_text)\n        return phonemes\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.768465Z","iopub.execute_input":"2025-05-07T20:53:04.768676Z","iopub.status.idle":"2025-05-07T20:53:04.786589Z","shell.execute_reply.started":"2025-05-07T20:53:04.768654Z","shell.execute_reply":"2025-05-07T20:53:04.785941Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class LyricsProcessor:\n    \"\"\"Class to process and align lyrics with audio\"\"\"\n    \n    def __init__(self, text_processor):\n        self.text_processor = text_processor\n    \n    def load_lyrics(self, lyrics_file):\n        \"\"\"Load lyrics from a file\"\"\"\n        with open(lyrics_file, 'r', encoding='utf-8') as f:\n            lyrics = f.read()\n        return lyrics\n    \n    def parse_lyrics_with_timestamps(self, lyrics_file):\n        \"\"\"\n        Parse lyrics with timestamps if available\n        Expected format: [MM:SS.ms] Lyric line\n        \"\"\"\n        lines = []\n        timestamps = []\n        \n        with open(lyrics_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                \n                # Try to extract timestamp\n                timestamp_match = re.match(r'\\[(\\d+):(\\d+)\\.(\\d+)\\](.*)', line)\n                if timestamp_match:\n                    minutes, seconds, milliseconds, text = timestamp_match.groups()\n                    time_in_seconds = int(minutes) * 60 + int(seconds) + int(milliseconds) / 1000\n                    timestamps.append(time_in_seconds)\n                    lines.append(text.strip())\n                else:\n                    # No timestamp, just add the line\n                    lines.append(line)\n                    # If we already have lines with timestamps, interpolate\n                    if timestamps:\n                        timestamps.append(-1)  # Mark for later interpolation\n        \n        return lines, timestamps\n    \n    def process_lyrics(self, lyrics_text):\n        \"\"\"Process lyrics text into phoneme and token sequences\"\"\"\n        lines = [line.strip() for line in lyrics_text.split('\\n') if line.strip()]\n        \n        phoneme_sequences = []\n        token_sequences = []\n        \n        for line in lines:\n            phonemes = self.text_processor.get_phoneme_sequence(line)\n            tokens = self.text_processor.text_to_sequence(line)\n            \n            phoneme_sequences.append(phonemes)\n            token_sequences.append(tokens)\n        \n        return {\n            'lines': lines,\n            'phoneme_sequences': phoneme_sequences,\n            'token_sequences': token_sequences\n        }\n    \n    def estimate_alignment(self, audio_length, num_lines):\n        \"\"\"\n        Estimate alignment between lyrics and audio when timestamps aren't available\n        Returns estimated timestamp for each line in seconds\n        \"\"\"\n        # Simple linear interpolation\n        return np.linspace(0, audio_length, num_lines + 1)[:-1]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.787387Z","iopub.execute_input":"2025-05-07T20:53:04.787653Z","iopub.status.idle":"2025-05-07T20:53:04.806232Z","shell.execute_reply.started":"2025-05-07T20:53:04.787630Z","shell.execute_reply":"2025-05-07T20:53:04.805673Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class SingerDataset(Dataset):\n    \"\"\"Memory-optimized dataset for singer voice data\"\"\"\n    \n    def __init__(self, audio_files, lyrics_files, audio_processor, lyrics_processor, \n                 max_len=500, cache_size=100, memory_limit_mb=12000):\n        self.audio_files = audio_files\n        self.lyrics_files = lyrics_files\n        self.audio_processor = audio_processor\n        self.lyrics_processor = lyrics_processor\n        self.max_len = max_len\n        self.cache_size = cache_size\n        self.memory_limit_mb = memory_limit_mb\n        \n        # Adjust cache size based on available memory\n        self._adjust_cache_size()\n        \n        # Instead of loading all data, just prepare metadata and file mapping\n        self.file_pairs = list(zip(audio_files, lyrics_files))\n        \n        # Validate files exist\n        self._validate_files()\n        \n        # Pre-index the dataset structure without loading full data\n        self.sample_indices = self._index_dataset()\n        \n        # Set up memory monitoring\n        self.last_memory_check = 0\n        self.memory_check_interval = 50  # Check memory every 50 getitem calls\n        \n        print(f\"Singer dataset initialized with {len(self.sample_indices)} segments across {len(self.file_pairs)} files\")\n        print(f\"Using a cache size of {self.cache_size} and memory limit of {self.memory_limit_mb}MB\")\n    \n    def _validate_files(self):\n        \"\"\"Verify that all files exist before starting\"\"\"\n        for audio_file, lyrics_file in self.file_pairs:\n            if not os.path.exists(audio_file):\n                raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n            if not os.path.exists(lyrics_file):\n                raise FileNotFoundError(f\"Lyrics file not found: {lyrics_file}\")\n    \n    def _index_dataset(self):\n        \"\"\"Create an index of all valid segments without loading their content\"\"\"\n        indices = []\n        \n        for file_idx, (audio_file, lyrics_file) in enumerate(self.file_pairs):\n            try:\n                # Process audio file minimally to get length\n                # Use existing process_audio_file but only extract the length\n                audio_features = self.audio_processor.process_audio_file(audio_file)\n                audio_length = len(audio_features['audio']) / self.audio_processor.sr\n                \n                # Free memory after extracting just the length\n                del audio_features\n                gc.collect()\n                \n                # Process lyrics to get line count\n                lyrics_text = self.lyrics_processor.load_lyrics(lyrics_file)\n                lyrics_data = self.lyrics_processor.process_lyrics(lyrics_text)\n                lines_count = len(lyrics_data['lines'])\n                \n                # Free memory after getting line count\n                del lyrics_data\n                gc.collect()\n                \n                # Estimate timestamps\n                timestamps = self.lyrics_processor.estimate_alignment(audio_length, lines_count)\n                \n                # Index each potential segment\n                for line_idx in range(lines_count):\n                    start_time = timestamps[line_idx]\n                    end_time = timestamps[line_idx+1] if line_idx+1 < len(timestamps) else audio_length\n                    \n                    start_frame = int(start_time * self.audio_processor.sr / self.audio_processor.hop_length)\n                    end_frame = int(end_time * self.audio_processor.sr / self.audio_processor.hop_length)\n                    \n                    # Skip if segment is too short\n                    if end_frame - start_frame >= 5:\n                        indices.append((file_idx, line_idx))\n                \n            except Exception as e:\n                print(f\"Error indexing {audio_file} and {lyrics_file}: {e}\")\n        \n        return indices\n    \n    @lru_cache(maxsize=16)  # Reduced cache size to limit memory usage\n    def _process_audio_file(self, audio_file):\n        \"\"\"Process audio file with caching\"\"\"\n        try:\n            features = self.audio_processor.process_audio_file(audio_file)\n            \n            # Create a copy to ensure we don't retain references to large objects\n            result = {\n                'audio': features['audio'].copy() if 'audio' in features else None,\n                'mel_spectrogram': features['mel_spectrogram'].copy() if 'mel_spectrogram' in features else None,\n                'pitch': features['pitch'].copy() if 'pitch' in features else None,\n                'energy': features['energy'].copy() if 'energy' in features else None\n            }\n            return result\n        except Exception as e:\n            print(f\"Error processing audio file {audio_file}: {e}\")\n            return None\n    \n    @lru_cache(maxsize=32)  # Cache a limited number of processed lyrics files\n    def _process_lyrics_file(self, lyrics_file):\n        \"\"\"Process lyrics file with caching\"\"\"\n        try:\n            lyrics_text = self.lyrics_processor.load_lyrics(lyrics_file)\n            return self.lyrics_processor.process_lyrics(lyrics_text)\n        except Exception as e:\n            print(f\"Error processing lyrics file {lyrics_file}: {e}\")\n            return None\n    \n    def _extract_segment(self, audio_features, lyrics_data, line_idx, audio_length):\n        \"\"\"Extract a specific segment from processed audio and lyrics\"\"\"\n        try:\n            # Ensure we have all required features\n            if (audio_features is None or \n                'mel_spectrogram' not in audio_features or \n                'pitch' not in audio_features or \n                'energy' not in audio_features):\n                return None\n                \n            # Estimate timestamps for this line\n            timestamps = self.lyrics_processor.estimate_alignment(\n                audio_length, len(lyrics_data['lines'])\n            )\n            \n            # Calculate start and end frames\n            start_time = timestamps[line_idx]\n            end_time = timestamps[line_idx+1] if line_idx+1 < len(timestamps) else audio_length\n            \n            start_frame = int(start_time * self.audio_processor.sr / self.audio_processor.hop_length)\n            end_frame = int(end_time * self.audio_processor.sr / self.audio_processor.hop_length)\n            \n            # Skip if segment is too short\n            if end_frame - start_frame < 5:\n                return None\n                \n            # Verify we have enough data in the features\n            mel_spec = audio_features['mel_spectrogram']\n            pitch = audio_features['pitch']\n            energy = audio_features['energy']\n            \n            if (mel_spec.shape[1] <= end_frame or \n                len(pitch) <= end_frame or \n                len(energy) <= end_frame):\n                # Our estimated end frame exceeds available data\n                end_frame = min(mel_spec.shape[1], len(pitch), len(energy))\n                if end_frame - start_frame < 5:  # Still check if segment is viable\n                    return None\n            \n            # Extract segment features with explicit copies to avoid memory leaks\n            try:\n                mel_segment = mel_spec[:, start_frame:end_frame].copy()\n                pitch_segment = pitch[start_frame:end_frame].copy()\n                energy_segment = energy[start_frame:end_frame].copy()\n            except IndexError:\n                print(f\"Index error for segment {line_idx}, frames {start_frame}:{end_frame}\")\n                return None\n            \n            # Skip if any feature is empty\n            if mel_segment.size == 0 or len(pitch_segment) == 0 or len(energy_segment) == 0:\n                return None\n            \n            # Pad or trim features to max_len\n            if mel_segment.shape[1] > self.max_len:\n                mel_segment = mel_segment[:, :self.max_len]\n                pitch_segment = pitch_segment[:self.max_len]\n                energy_segment = energy_segment[:self.max_len]\n            else:\n                pad_len = self.max_len - mel_segment.shape[1]\n                mel_segment = np.pad(mel_segment, ((0, 0), (0, pad_len)))\n                pitch_segment = np.pad(pitch_segment, (0, pad_len))\n                energy_segment = np.pad(energy_segment, (0, pad_len))\n            \n            # Create mask\n            mask = np.ones(self.max_len)\n            mask[mel_segment.shape[1]:] = 0\n            \n            return {\n                'text': lyrics_data['lines'][line_idx],\n                'phonemes': lyrics_data['phoneme_sequences'][line_idx],\n                'tokens': lyrics_data['token_sequences'][line_idx],\n                'mel_spectrogram': mel_segment,\n                'pitch': pitch_segment,\n                'energy': energy_segment,\n                'mask': mask\n            }\n            \n        except Exception as e:\n            print(f\"Error extracting segment {line_idx}: {e}\")\n            return None\n    \n    def __len__(self):\n        return len(self.sample_indices)\n    \n    def _adjust_cache_size(self):\n        \"\"\"Dynamically adjust cache size based on available memory\"\"\"\n        try:\n            available_memory = psutil.virtual_memory().available / (1024 * 1024)  # Convert to MB\n            if available_memory < self.memory_limit_mb * 0.2:  # If less than 20% of limit is available\n                self.cache_size = max(10, self.cache_size // 2)  # Reduce cache size\n            elif available_memory > self.memory_limit_mb * 0.8:  # If more than 80% of limit is available\n                self.cache_size = min(200, self.cache_size * 2)  # Increase cache size\n        except:\n            # Default to conservative cache size if psutil isn't available\n            self.cache_size = 50\n            \n    def _check_memory_usage(self):\n        \"\"\"Check memory usage and clear caches if needed\"\"\"\n        try:\n            used_memory_mb = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n            if used_memory_mb > self.memory_limit_mb * 0.8:  # Using more than 80% of limit\n                print(f\"Memory usage high ({used_memory_mb:.1f}MB), clearing caches\")\n                self.cleanup()\n                return True\n        except:\n            pass  # Silently fail if psutil isn't available\n        return False\n\n    def __getitem__(self, idx):\n        # Periodically check memory usage\n        if idx - self.last_memory_check > self.memory_check_interval:\n            self.last_memory_check = idx\n            memory_cleared = self._check_memory_usage()\n        \n        try:\n            # Get file and line indices\n            file_idx, line_idx = self.sample_indices[idx]\n            audio_file, lyrics_file = self.file_pairs[file_idx]\n            \n            # Process files as needed\n            audio_features = self._process_audio_file(audio_file)\n            if audio_features is None:\n                return self._get_empty_item()\n                \n            lyrics_data = self._process_lyrics_file(lyrics_file)\n            if lyrics_data is None:\n                return self._get_empty_item()\n            \n            # Extract the specific segment\n            audio_length = len(audio_features['audio']) / self.audio_processor.sr if 'audio' in audio_features else 0\n            if audio_length == 0:\n                return self._get_empty_item()\n                \n            segment = self._extract_segment(audio_features, lyrics_data, line_idx, audio_length)\n            \n            if segment is None:\n                # Return an empty placeholder if extraction failed\n                return self._get_empty_item()\n            \n            # Convert to tensors\n            tokens = torch.tensor(segment['tokens'], dtype=torch.long)\n            mel_spec = torch.tensor(segment['mel_spectrogram'], dtype=torch.float)\n            pitch = torch.tensor(segment['pitch'], dtype=torch.float)\n            energy = torch.tensor(segment['energy'], dtype=torch.float)\n            mask = torch.tensor(segment['mask'], dtype=torch.float)\n            \n            # Force garbage collection periodically\n            if idx % self.cache_size == 0:\n                gc.collect()\n            \n            return {\n                'text': segment['text'],\n                'phonemes': segment['phonemes'],\n                'tokens': tokens,\n                'mel_spectrogram': mel_spec,\n                'pitch': pitch, \n                'energy': energy,\n                'mask': mask\n            }\n        \n        except Exception as e:\n            print(f\"Error in __getitem__ for index {idx}: {e}\")\n            return self._get_empty_item()\n    \n    def _get_empty_item(self):\n        \"\"\"Return an empty placeholder item for error cases\"\"\"\n        return {\n            'text': \"\",\n            'phonemes': [],\n            'tokens': torch.zeros(1, dtype=torch.long),\n            'mel_spectrogram': torch.zeros((80, self.max_len), dtype=torch.float),\n            'pitch': torch.zeros(self.max_len, dtype=torch.float),\n            'energy': torch.zeros(self.max_len, dtype=torch.float),\n            'mask': torch.zeros(self.max_len, dtype=torch.float)\n        }\n    \n    def cleanup(self):\n        \"\"\"Explicitly clear caches and run garbage collection\"\"\"\n        self._process_audio_file.cache_clear()\n        self._process_lyrics_file.cache_clear()\n        gc.collect()\n        gc.collect()  # Run twice to ensure collection of cyclical references\n        \n    def reduce_memory_usage(self):\n        \"\"\"More aggressive memory reduction when running out of RAM\"\"\"\n        # Clear all caches\n        self.cleanup()\n        \n        # Reduce cache sizes\n        self.cache_size = max(10, self.cache_size // 2)\n        \n        # Force Python to return memory to OS if possible\n        try:\n            import ctypes\n            ctypes.CDLL('libc.so.6').malloc_trim(0)\n        except:\n            pass\n            \n    def __del__(self):\n        \"\"\"Cleanup when object is destroyed\"\"\"\n        self.cleanup()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:59:30.669246Z","iopub.execute_input":"2025-05-07T20:59:30.670143Z","iopub.status.idle":"2025-05-07T20:59:30.702118Z","shell.execute_reply.started":"2025-05-07T20:59:30.670117Z","shell.execute_reply":"2025-05-07T20:59:30.701430Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    \"\"\"Text encoder module\"\"\"\n    \n    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, n_layers=3, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_dim,\n            num_layers=n_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if n_layers > 1 else 0\n        )\n        self.projection = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        # x shape: (batch_size, seq_len)\n        embedded = self.dropout(self.embedding(x))\n        # embedded shape: (batch_size, seq_len, embed_dim)\n        \n        outputs, _ = self.lstm(embedded)\n        # outputs shape: (batch_size, seq_len, hidden_dim * 2)\n        \n        outputs = self.projection(outputs)\n        # outputs shape: (batch_size, seq_len, hidden_dim)\n        \n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.829953Z","iopub.execute_input":"2025-05-07T20:53:04.830135Z","iopub.status.idle":"2025-05-07T20:53:04.848099Z","shell.execute_reply.started":"2025-05-07T20:53:04.830120Z","shell.execute_reply":"2025-05-07T20:53:04.847540Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class VarianceAdaptor(nn.Module):\n    \"\"\"Adapts variance information (pitch, energy) to the model\"\"\"\n    \n    def __init__(self, hidden_dim=512, kernel_size=3, dropout=0.1):\n        super().__init__()\n        \n        # Pitch predictor\n        self.pitch_predictor = nn.Sequential(\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(dropout),\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n        # Energy predictor\n        self.energy_predictor = nn.Sequential(\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(dropout),\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n        # Duration predictor (for training)\n        self.duration_predictor = nn.Sequential(\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(dropout),\n            nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_dim),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n        # Embeddings for transforming predicted values\n        self.pitch_embedding = nn.Linear(1, hidden_dim)\n        self.energy_embedding = nn.Linear(1, hidden_dim)\n    \n    def forward(self, x, pitch=None, energy=None):\n        # x shape: (batch_size, seq_len, hidden_dim)\n        \n        # Transpose for Conv1d\n        x_conv = x.transpose(1, 2)\n        \n        # Predict pitch and energy if not provided\n        if pitch is None:\n            pitch_pred = self.pitch_predictor(x_conv).transpose(1, 2)\n        else:\n            pitch_pred = pitch.unsqueeze(-1)\n        \n        if energy is None:\n            energy_pred = self.energy_predictor(x_conv).transpose(1, 2)\n        else:\n            energy_pred = energy.unsqueeze(-1)\n        \n        # Get embeddings\n        pitch_embedding = self.pitch_embedding(pitch_pred)\n        energy_embedding = self.energy_embedding(energy_pred)\n        \n        # Add variance embeddings to x\n        outputs = x + pitch_embedding + energy_embedding\n        \n        return outputs, pitch_pred.squeeze(-1), energy_pred.squeeze(-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.848834Z","iopub.execute_input":"2025-05-07T20:53:04.849462Z","iopub.status.idle":"2025-05-07T20:53:04.867775Z","shell.execute_reply.started":"2025-05-07T20:53:04.849442Z","shell.execute_reply":"2025-05-07T20:53:04.867014Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \"\"\"Decoder to generate mel spectrograms\"\"\"\n    \n    def __init__(self, hidden_dim=512, n_mels=80, kernel_size=5, n_layers=4, dropout=0.1):\n        super().__init__()\n        \n        self.pre_net = nn.Sequential(\n            nn.Linear(n_mels, hidden_dim//2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim//2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.attention_rnn = nn.LSTMCell(hidden_dim * 2, hidden_dim)\n        \n        self.decoder_rnn = nn.LSTMCell(hidden_dim, hidden_dim)\n        \n        self.attention = nn.Linear(hidden_dim * 2, 1, bias=False)\n        \n        self.projection = nn.Linear(hidden_dim, n_mels)\n        \n        self.post_net = nn.Sequential(\n            nn.Conv1d(n_mels, hidden_dim, kernel_size, padding=kernel_size//2),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Tanh(),\n            nn.Dropout(dropout),\n            *[nn.Sequential(\n                nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2),\n                nn.BatchNorm1d(hidden_dim),\n                nn.Tanh(),\n                nn.Dropout(dropout)\n            ) for _ in range(n_layers - 2)],\n            nn.Conv1d(hidden_dim, n_mels, kernel_size, padding=kernel_size//2),\n            nn.BatchNorm1d(n_mels),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, encoder_outputs, mel_targets=None, teacher_forcing_ratio=0.5):\n        # encoder_outputs shape: (batch_size, seq_len, hidden_dim)\n        batch_size, seq_len, hidden_dim = encoder_outputs.shape\n        \n        # Initialize decoder states\n        attn_hidden = torch.zeros(batch_size, hidden_dim).to(encoder_outputs.device)\n        attn_cell = torch.zeros(batch_size, hidden_dim).to(encoder_outputs.device)\n        \n        decoder_hidden = torch.zeros(batch_size, hidden_dim).to(encoder_outputs.device)\n        decoder_cell = torch.zeros(batch_size, hidden_dim).to(encoder_outputs.device)\n        \n        # Initialize attention weights\n        attn_weights = torch.zeros(batch_size, seq_len).to(encoder_outputs.device)\n        \n        # Initialize first input as zeros\n        decoder_input = torch.zeros(batch_size, encoder_outputs.shape[-1]).to(encoder_outputs.device)\n        \n        # Initialize outputs\n        mel_outputs = []\n        \n        # Generate target sequence length\n        target_len = mel_targets.shape[1] if mel_targets is not None else 200\n        \n        for t in range(target_len):\n            # Use teacher forcing\n            if mel_targets is not None and torch.rand(1) < teacher_forcing_ratio:\n                current_input = mel_targets[:, t, :] if t < mel_targets.shape[1] else decoder_input\n            else:\n                current_input = decoder_input\n            \n            # Apply pre-net\n            prenet_out = self.pre_net(current_input)\n            \n            # Attention RNN\n            attn_input = torch.cat([prenet_out, decoder_hidden], dim=1)\n            attn_hidden, attn_cell = self.attention_rnn(attn_input, (attn_hidden, attn_cell))\n            \n            # Calculate attention\n            attn_energy = self.attention(\n                torch.cat([attn_hidden.unsqueeze(1).expand(-1, seq_len, -1), \n                          encoder_outputs], dim=2)\n            ).squeeze(-1)\n            \n            attn_weights = F.softmax(attn_energy, dim=1)\n            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n            \n            # Decoder RNN\n            decoder_input = torch.cat([attn_hidden, context], dim=1)\n            decoder_hidden, decoder_cell = self.decoder_rnn(decoder_input, (decoder_hidden, decoder_cell))\n            \n            # Project to get mel output\n            mel_output = self.projection(decoder_hidden)\n            mel_outputs.append(mel_output)\n            \n            # Use this output as next input\n            decoder_input = mel_output\n        \n        # Stack mel outputs\n        mel_outputs = torch.stack(mel_outputs, dim=1)\n        \n        # Apply post-net and add residual connection\n        post_output = self.post_net(mel_outputs.transpose(1, 2)).transpose(1, 2)\n        mel_outputs_postnet = mel_outputs + post_output\n        \n        return mel_outputs, mel_outputs_postnet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.868519Z","iopub.execute_input":"2025-05-07T20:53:04.868713Z","iopub.status.idle":"2025-05-07T20:53:04.888078Z","shell.execute_reply.started":"2025-05-07T20:53:04.868699Z","shell.execute_reply":"2025-05-07T20:53:04.887490Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class VoiceCloning(nn.Module):\n    \"\"\"Complete voice cloning model\"\"\"\n    \n    def __init__(self, vocab_size, n_mels=80, hidden_dim=512):\n        super().__init__()\n        \n        self.text_encoder = TextEncoder(vocab_size, hidden_dim=hidden_dim)\n        self.variance_adaptor = VarianceAdaptor(hidden_dim=hidden_dim)\n        self.decoder = Decoder(hidden_dim=hidden_dim, n_mels=n_mels)\n    \n    def forward(self, tokens, pitch=None, energy=None, mel_targets=None, teacher_forcing_ratio=0.5):\n        # Encode text\n        encoder_outputs = self.text_encoder(tokens)\n        \n        # Apply variance adaptor\n        variance_outputs, pitch_pred, energy_pred = self.variance_adaptor(\n            encoder_outputs, pitch, energy\n        )\n        \n        # Generate mel spectrograms\n        mel_outputs, mel_outputs_postnet = self.decoder(\n            variance_outputs, mel_targets, teacher_forcing_ratio\n        )\n        \n        return {\n            'mel_outputs': mel_outputs,\n            'mel_outputs_postnet': mel_outputs_postnet,\n            'pitch_pred': pitch_pred,\n            'energy_pred': energy_pred\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.888755Z","iopub.execute_input":"2025-05-07T20:53:04.888972Z","iopub.status.idle":"2025-05-07T20:53:04.905679Z","shell.execute_reply.started":"2025-05-07T20:53:04.888949Z","shell.execute_reply":"2025-05-07T20:53:04.905081Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class WaveNetVocoder(nn.Module):\n    \"\"\"WaveNet-based vocoder to convert mel spectrograms to waveform\"\"\"\n    \n    def __init__(self, n_mels=80, channels=256, kernel_size=3, n_layers=24, dilation_cycle=12):\n        super().__init__()\n        \n        # Input layer\n        self.input_conv = nn.Conv1d(n_mels, channels, kernel_size=1)\n        \n        # Residual blocks with dilated convolutions\n        self.residual_blocks = nn.ModuleList([\n            nn.ModuleDict({\n                'dilated_conv': nn.Conv1d(\n                    channels, 2 * channels, kernel_size, \n                    dilation=2 ** (i % dilation_cycle), padding='same'\n                ),\n                'res_conv': nn.Conv1d(channels, channels, kernel_size=1),\n                'skip_conv': nn.Conv1d(channels, channels, kernel_size=1)\n            }) for i in range(n_layers)\n        ])\n        \n        # Output layers\n        self.output_layers = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv1d(channels, channels, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv1d(channels, 1, kernel_size=1),\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        # x shape: (batch_size, n_mels, time_steps)\n        \n        # Apply input layer\n        x = self.input_conv(x)\n        \n        # Initialize skip connections\n        skip_connections = 0\n        \n        # Apply residual blocks\n        for block in self.residual_blocks:\n            # Dilated convolution\n            h = block['dilated_conv'](x)\n            h_tanh, h_sigmoid = torch.split(h, h.size(1) // 2, dim=1)\n            h = torch.tanh(h_tanh) * torch.sigmoid(h_sigmoid)\n            \n            # Residual and skip connections\n            res = block['res_conv'](h)\n            skip = block['skip_conv'](h)\n            \n            x = x + res\n            skip_connections = skip_connections + skip\n        \n        # Apply output layers\n        output = self.output_layers(skip_connections)\n        \n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.906444Z","iopub.execute_input":"2025-05-07T20:53:04.906649Z","iopub.status.idle":"2025-05-07T20:53:04.923836Z","shell.execute_reply.started":"2025-05-07T20:53:04.906626Z","shell.execute_reply":"2025-05-07T20:53:04.923189Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def create_alignment(text, mel):\n    \"\"\"Create a simple alignment between text and mel frames\"\"\"\n    text_len = text.shape[1]\n    mel_len = mel.shape[2]\n    \n    # Simple linear alignment\n    alignment = torch.zeros(1, text_len, mel_len)\n    step = mel_len / text_len\n    \n    for i in range(text_len):\n        start = int(i * step)\n        end = int((i + 1) * step)\n        alignment[0, i, start:end] = 1.0\n    \n    return alignment\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.926420Z","iopub.execute_input":"2025-05-07T20:53:04.926630Z","iopub.status.idle":"2025-05-07T20:53:04.944494Z","shell.execute_reply.started":"2025-05-07T20:53:04.926614Z","shell.execute_reply":"2025-05-07T20:53:04.943853Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Collate function for DataLoader\"\"\"\n    # Get max lengths\n    max_token_len = max([len(item['tokens']) for item in batch])\n    \n    # Initialize tensors\n    tokens = torch.zeros(len(batch), max_token_len, dtype=torch.long)\n    mel_specs = torch.stack([item['mel_spectrogram'] for item in batch])\n    pitches = torch.stack([item['pitch'] for item in batch])\n    energies = torch.stack([item['energy'] for item in batch])\n    masks = torch.stack([item['mask'] for item in batch])\n    \n    # Fill tokens tensor\n    for i, item in enumerate(batch):\n        tokens[i, :len(item['tokens'])] = item['tokens']\n    \n    return {\n        'texts': [item['text'] for item in batch],\n        'phonemes': [item['phonemes'] for item in batch],\n        'tokens': tokens,\n        'mel_spectrograms': mel_specs,\n        'pitches': pitches,\n        'energies': energies,\n        'masks': masks\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.945068Z","iopub.execute_input":"2025-05-07T20:53:04.945264Z","iopub.status.idle":"2025-05-07T20:53:04.958392Z","shell.execute_reply.started":"2025-05-07T20:53:04.945248Z","shell.execute_reply":"2025-05-07T20:53:04.957805Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def train_model(model, vocoder, train_loader, valid_loader, device, epochs=100):\n    \"\"\"Train the voice cloning model\"\"\"\n    # Define optimizers\n    optimizer = Adam(model.parameters(), lr=0.001)\n    vocoder_optimizer = Adam(vocoder.parameters(), lr=0.0005)\n    \n    # Define loss functions\n    mel_loss_fn = nn.L1Loss()\n    pitch_loss_fn = nn.MSELoss()\n    energy_loss_fn = nn.MSELoss()\n    waveform_loss_fn = nn.L1Loss()\n    \n    # Learning rate schedulers\n    model_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n    )\n    vocoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        vocoder_optimizer, mode='min', factor=0.5, patience=8, verbose=True\n    )\n    \n    # TensorBoard writer\n    writer = SummaryWriter('runs/voice_cloning')\n    \n    # Training loop\n    best_valid_loss = float('inf')\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        vocoder.train()\n        \n        train_loss = 0\n        train_mel_loss = 0\n        train_pitch_loss = 0\n        train_energy_loss = 0\n        # Continue training loop\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]'):\n            # Move batch to device\n            tokens = batch['tokens'].to(device)\n            mel_specs = batch['mel_spectrograms'].to(device)\n            pitches = batch['pitches'].to(device)\n            energies = batch['energies'].to(device)\n            masks = batch['masks'].to(device)\n            \n            # Forward pass for voice cloning model\n            outputs = model(tokens, pitches, energies, mel_specs)\n            \n            # Apply mask to outputs\n            mel_outputs = outputs['mel_outputs'] * masks.unsqueeze(-1)\n            mel_outputs_postnet = outputs['mel_outputs_postnet'] * masks.unsqueeze(-1)\n            pitch_pred = outputs['pitch_pred'] * masks\n            energy_pred = outputs['energy_pred'] * masks\n            \n            # Calculate losses\n            mel_loss = mel_loss_fn(mel_outputs, mel_specs) + mel_loss_fn(mel_outputs_postnet, mel_specs)\n            pitch_loss = pitch_loss_fn(pitch_pred, pitches)\n            energy_loss = energy_loss_fn(energy_pred, energies)\n            \n            # Combine losses for voice cloning model\n            loss = mel_loss + pitch_loss + energy_loss\n            \n            # Backward pass and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            # Forward pass for vocoder\n            if epoch >= 5:  # Start training vocoder after a few epochs\n                # Generate audio from mel spectrogram\n                mel_specs_for_vocoder = mel_specs.transpose(1, 2)  # (batch, time_steps, n_mels) -> (batch, n_mels, time_steps)\n                waveform_pred = vocoder(mel_specs_for_vocoder)\n                \n                # Load target waveform (placeholder - would need actual target audio)\n                # This is a simplified placeholder; in practice, you'd align the waveform with the mel frames\n                target_waveform = torch.randn_like(waveform_pred)  # Placeholder\n                \n                # Vocoder loss\n                vocoder_loss = waveform_loss_fn(waveform_pred, target_waveform)\n                \n                # Backward pass and optimize vocoder\n                vocoder_optimizer.zero_grad()\n                vocoder_loss.backward()\n                torch.nn.utils.clip_grad_norm_(vocoder.parameters(), 1.0)\n                vocoder_optimizer.step()\n            \n            # Accumulate losses\n            train_loss += loss.item()\n            train_mel_loss += mel_loss.item()\n            train_pitch_loss += pitch_loss.item()\n            train_energy_loss += energy_loss.item()\n        \n        # Calculate average training losses\n        train_loss /= len(train_loader)\n        train_mel_loss /= len(train_loader)\n        train_pitch_loss /= len(train_loader)\n        train_energy_loss /= len(train_loader)\n        \n        # Validation\n        model.eval()\n        vocoder.eval()\n        \n        valid_loss = 0\n        valid_mel_loss = 0\n        valid_pitch_loss = 0\n        valid_energy_loss = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(valid_loader, desc=f'Epoch {epoch+1}/{epochs} [Valid]'):\n                # Move batch to device\n                tokens = batch['tokens'].to(device)\n                mel_specs = batch['mel_spectrograms'].to(device)\n                pitches = batch['pitches'].to(device)\n                energies = batch['energies'].to(device)\n                masks = batch['masks'].to(device)\n                \n                # Forward pass\n                outputs = model(tokens, pitches, energies, mel_specs)\n                \n                # Apply mask to outputs\n                mel_outputs = outputs['mel_outputs'] * masks.unsqueeze(-1)\n                mel_outputs_postnet = outputs['mel_outputs_postnet'] * masks.unsqueeze(-1)\n                pitch_pred = outputs['pitch_pred'] * masks\n                energy_pred = outputs['energy_pred'] * masks\n                \n                # Calculate losses\n                mel_loss = mel_loss_fn(mel_outputs, mel_specs) + mel_loss_fn(mel_outputs_postnet, mel_specs)\n                pitch_loss = pitch_loss_fn(pitch_pred, pitches)\n                energy_loss = energy_loss_fn(energy_pred, energies)\n                \n                # Combine losses\n                loss = mel_loss + pitch_loss + energy_loss\n                \n                # Accumulate losses\n                valid_loss += loss.item()\n                valid_mel_loss += mel_loss.item()\n                valid_pitch_loss += pitch_loss.item()\n                valid_energy_loss += energy_loss.item()\n        \n        # Calculate average validation losses\n        valid_loss /= len(valid_loader)\n        valid_mel_loss /= len(valid_loader)\n        valid_pitch_loss /= len(valid_loader)\n        valid_energy_loss /= len(valid_loader)\n        \n        # Update learning rate schedulers\n        model_scheduler.step(valid_loss)\n        if epoch >= 5:\n            vocoder_scheduler.step(valid_loss)\n        \n        # Log to TensorBoard\n        writer.add_scalar('Loss/train', train_loss, epoch)\n        writer.add_scalar('Loss/valid', valid_loss, epoch)\n        writer.add_scalar('MelLoss/train', train_mel_loss, epoch)\n        writer.add_scalar('MelLoss/valid', valid_mel_loss, epoch)\n        writer.add_scalar('PitchLoss/train', train_pitch_loss, epoch)\n        writer.add_scalar('PitchLoss/valid', valid_pitch_loss, epoch)\n        writer.add_scalar('EnergyLoss/train', train_energy_loss, epoch)\n        writer.add_scalar('EnergyLoss/valid', valid_energy_loss, epoch)\n        \n        # Print progress\n        print(f'Epoch {epoch+1}/{epochs}:')\n        print(f'  Train Loss: {train_loss:.4f} (Mel: {train_mel_loss:.4f}, Pitch: {train_pitch_loss:.4f}, Energy: {train_energy_loss:.4f})')\n        print(f'  Valid Loss: {valid_loss:.4f} (Mel: {valid_mel_loss:.4f}, Pitch: {valid_pitch_loss:.4f}, Energy: {valid_energy_loss:.4f})')\n        \n        # Save checkpoint if validation loss improved\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'vocoder_state_dict': vocoder.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'vocoder_optimizer_state_dict': vocoder_optimizer.state_dict(),\n                'loss': valid_loss,\n            }, f'checkpoints/voice_cloning_best.pt')\n            print(f'  New best model saved!')\n        \n        # Save latest model\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'vocoder_state_dict': vocoder.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'vocoder_optimizer_state_dict': vocoder_optimizer.state_dict(),\n            'loss': valid_loss,\n        }, f'checkpoints/voice_cloning_latest.pt')\n    \n    # Close TensorBoard writer\n    writer.close()\n    \n    return model, vocoder\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.959251Z","iopub.execute_input":"2025-05-07T20:53:04.959503Z","iopub.status.idle":"2025-05-07T20:53:04.977298Z","shell.execute_reply.started":"2025-05-07T20:53:04.959477Z","shell.execute_reply":"2025-05-07T20:53:04.976481Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\nclass SongGenerator:\n    \"\"\"Class to generate songs from text input\"\"\"\n    \n    def __init__(self, model, vocoder, text_processor, audio_processor, device):\n        self.model = model\n        self.vocoder = vocoder\n        self.text_processor = text_processor\n        self.audio_processor = audio_processor\n        self.device = device\n        \n        # Set models to evaluation mode\n        self.model.eval()\n        self.vocoder.eval()\n    \n    def generate_from_text(self, text, pitch_control=1.0, energy_control=1.0, speed_control=1.0):\n        \"\"\"Generate a song from input text\"\"\"\n        # Process text\n        tokens = self.text_processor.text_to_sequence(text)\n        tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(self.device)\n        \n        # Generate with the model\n        with torch.no_grad():\n            # Forward pass through model\n            outputs = self.model(tokens)\n            \n            # Apply control parameters\n            pitch_pred = outputs['pitch_pred'] * pitch_control\n            energy_pred = outputs['energy_pred'] * energy_control\n            \n            # Adjust speed by repeating/skipping frames\n            mel_outputs = outputs['mel_outputs_postnet']\n            if speed_control != 1.0:\n                target_length = int(mel_outputs.shape[1] / speed_control)\n                mel_outputs = F.interpolate(\n                    mel_outputs.transpose(1, 2), \n                    size=target_length, \n                    mode='linear'\n                ).transpose(1, 2)\n            \n            # Generate waveform with vocoder\n            waveform = self.vocoder(mel_outputs.transpose(1, 2)).squeeze().cpu().numpy()\n        \n        return {\n            'waveform': waveform,\n            'mel_spectrogram': mel_outputs.squeeze().cpu().numpy(),\n            'pitch': pitch_pred.squeeze().cpu().numpy(),\n            'energy': energy_pred.squeeze().cpu().numpy()\n        }\n    \n    def save_audio(self, waveform, file_path, sample_rate=22050):\n        \"\"\"Save audio to file\"\"\"\n        sf.write(file_path, waveform, sample_rate)\n    \n    def plot_features(self, mel_spec, pitch, energy):\n        \"\"\"Plot generated features\"\"\"\n        fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n        \n        # Plot mel spectrogram\n        im = axes[0].imshow(mel_spec, aspect='auto', origin='lower')\n        axes[0].set_title('Mel Spectrogram')\n        axes[0].set_ylabel('Mel Bins')\n        fig.colorbar(im, ax=axes[0])\n        \n        # Plot pitch\n        axes[1].plot(pitch)\n        axes[1].set_title('Pitch')\n        axes[1].set_ylabel('Normalized F0')\n        \n        # Plot energy\n        axes[2].plot(energy)\n        axes[2].set_title('Energy')\n        axes[2].set_ylabel('Normalized Energy')\n        axes[2].set_xlabel('Frames')\n        \n        plt.tight_layout()\n        return fig\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.978127Z","iopub.execute_input":"2025-05-07T20:53:04.978389Z","iopub.status.idle":"2025-05-07T20:53:04.995419Z","shell.execute_reply.started":"2025-05-07T20:53:04.978365Z","shell.execute_reply":"2025-05-07T20:53:04.994633Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Set paths\ndata_dir = 'data/'\naudio_dir = '/kaggle/input/audio-dataset'\nlyrics_dir = '/kaggle/input/lyrics'\noutput_dir = 'output/'\ncheckpoint_dir = 'checkpoints/'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:04.996224Z","iopub.execute_input":"2025-05-07T20:53:04.996490Z","iopub.status.idle":"2025-05-07T20:53:05.011446Z","shell.execute_reply.started":"2025-05-07T20:53:04.996467Z","shell.execute_reply":"2025-05-07T20:53:05.010841Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Create directories if they don't exist\nos.makedirs(audio_dir, exist_ok=True)\nos.makedirs(lyrics_dir, exist_ok=True)\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:05.012102Z","iopub.execute_input":"2025-05-07T20:53:05.012331Z","iopub.status.idle":"2025-05-07T20:53:05.029408Z","shell.execute_reply.started":"2025-05-07T20:53:05.012315Z","shell.execute_reply":"2025-05-07T20:53:05.028758Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Initialize processors\naudio_processor = AudioProcessor()\ntext_processor = TextProcessor()\nlyrics_processor = LyricsProcessor(text_processor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:05.029956Z","iopub.execute_input":"2025-05-07T20:53:05.030128Z","iopub.status.idle":"2025-05-07T20:53:07.808294Z","shell.execute_reply.started":"2025-05-07T20:53:05.030116Z","shell.execute_reply":"2025-05-07T20:53:07.807435Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Get file paths\naudio_files = sorted(glob.glob(os.path.join(audio_dir, '*.wav')))\nlyrics_files = sorted(glob.glob(os.path.join(lyrics_dir, '*.txt')))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:07.809184Z","iopub.execute_input":"2025-05-07T20:53:07.809935Z","iopub.status.idle":"2025-05-07T20:53:07.821101Z","shell.execute_reply.started":"2025-05-07T20:53:07.809906Z","shell.execute_reply":"2025-05-07T20:53:07.820511Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Ensure we have matching files\nif len(audio_files) != len(lyrics_files):\n    print(f\"Warning: Number of audio files ({len(audio_files)}) doesn't match number of lyrics files ({len(lyrics_files)})\")\n    # Use the minimum number\n    n_files = min(len(audio_files), len(lyrics_files))\n    audio_files = audio_files[:n_files]\n    lyrics_files = lyrics_files[:n_files]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:07.821886Z","iopub.execute_input":"2025-05-07T20:53:07.822169Z","iopub.status.idle":"2025-05-07T20:53:07.828492Z","shell.execute_reply.started":"2025-05-07T20:53:07.822127Z","shell.execute_reply":"2025-05-07T20:53:07.827931Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Split data\ntrain_audio, valid_audio, train_lyrics, valid_lyrics = train_test_split(\n    audio_files, lyrics_files, test_size=0.2, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:07.829124Z","iopub.execute_input":"2025-05-07T20:53:07.829359Z","iopub.status.idle":"2025-05-07T20:53:07.844448Z","shell.execute_reply.started":"2025-05-07T20:53:07.829341Z","shell.execute_reply":"2025-05-07T20:53:07.843863Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = SingerDataset(train_audio, train_lyrics, audio_processor, lyrics_processor)\nvalid_dataset = SingerDataset(valid_audio, valid_lyrics, audio_processor, lyrics_processor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:59:39.873128Z","iopub.execute_input":"2025-05-07T20:59:39.873676Z","execution_failed":"2025-05-07T21:06:02.685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=8, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=2\n)\n\nvalid_loader = DataLoader(\n    valid_dataset, \n    batch_size=8, \n    shuffle=False, \n    collate_fn=collate_fn,\n    num_workers=2\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:07.859709Z","iopub.execute_input":"2025-05-07T20:53:07.859914Z","iopub.status.idle":"2025-05-07T20:53:08.007866Z","shell.execute_reply.started":"2025-05-07T20:53:07.859899Z","shell.execute_reply":"2025-05-07T20:53:08.006980Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_334/3740079084.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_loader = DataLoader(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"],"ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"\n# Initialize models\nvocab_size = len(text_processor.tokenizer)\nmodel = VoiceCloning(vocab_size, n_mels=80, hidden_dim=512).to(device)\nvocoder = WaveNetVocoder(n_mels=80).to(device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:08.008307Z","iopub.status.idle":"2025-05-07T20:53:08.008552Z","shell.execute_reply.started":"2025-05-07T20:53:08.008432Z","shell.execute_reply":"2025-05-07T20:53:08.008442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Check if checkpoint exists\ncheckpoint_path = os.path.join(checkpoint_dir, 'voice_cloning_latest.pt')\nif os.path.exists(checkpoint_path):\n    print(f\"Loading checkpoint from {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    vocoder.load_state_dict(checkpoint['vocoder_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resuming from epoch {start_epoch}\")\nelse:\n    start_epoch = 0\n    print(\"Starting training from scratch\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:08.009692Z","iopub.status.idle":"2025-05-07T20:53:08.010443Z","shell.execute_reply.started":"2025-05-07T20:53:08.010304Z","shell.execute_reply":"2025-05-07T20:53:08.010319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train models\nmodel, vocoder = train_model(\n    model, \n    vocoder, \n    train_loader, \n    valid_loader, \n    device, \n    epochs=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:08.011338Z","iopub.status.idle":"2025-05-07T20:53:08.011621Z","shell.execute_reply.started":"2025-05-07T20:53:08.011472Z","shell.execute_reply":"2025-05-07T20:53:08.011488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize song generator\nsong_generator = SongGenerator(model, vocoder, text_processor, audio_processor, device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:08.012943Z","iopub.status.idle":"2025-05-07T20:53:08.013292Z","shell.execute_reply.started":"2025-05-07T20:53:08.013115Z","shell.execute_reply":"2025-05-07T20:53:08.013132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Generate a song from text\nsample_text = \"Mein tumhare saath hoon, har pal har lamha\"\ngenerated = song_generator.generate_from_text(\n    sample_text, \n    pitch_control=1.0,\n    energy_control=1.0,\n    speed_control=1.0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:08.014535Z","iopub.status.idle":"2025-05-07T20:53:08.014875Z","shell.execute_reply.started":"2025-05-07T20:53:08.014709Z","shell.execute_reply":"2025-05-07T20:53:08.014725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save generated audio\noutput_path = os.path.join(output_dir, 'generated_song.wav')\nsong_generator.save_audio(generated['waveform'], output_path)\nprint(f\"Generated song saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:08.016550Z","iopub.status.idle":"2025-05-07T20:53:08.016874Z","shell.execute_reply.started":"2025-05-07T20:53:08.016723Z","shell.execute_reply":"2025-05-07T20:53:08.016738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot features\nfig = song_generator.plot_features(\n    generated['mel_spectrogram'],\n    generated['pitch'],\n    generated['energy']\n)\nfig.savefig(os.path.join(output_dir, 'feature_plot.png'))\n\nprint(\"Voice cloning complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T20:53:08.017901Z","iopub.status.idle":"2025-05-07T20:53:08.018231Z","shell.execute_reply.started":"2025-05-07T20:53:08.018053Z","shell.execute_reply":"2025-05-07T20:53:08.018071Z"}},"outputs":[],"execution_count":null}]}